{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kili Tutorial: How to leverage Counterfactually augmented data to have a more robust model\n",
    "\n",
    "This recipe is inspired by the paper *Learning the Difference that Makes a Difference with Counterfactually-Augmented Data*, that you can find here on [arXiv](https://arxiv.org/abs/1909.12434)\n",
    "\n",
    "In this study, the authors point out the difficulty for Machine Learning models to generalize the classification rules learned, because their decision rules, described as 'spurious patternes', often miss the key elements that affects most the class of a text. They thus decided to delete what can be considered as a confusion factor, by changing the label of an asset at the same time as changing the minimum amount of words so those **key-words** would be much easier for the model to spot.\n",
    "\n",
    "We'll see in this tutorial :\n",
    "1. How to create a project in Kili, both for [IMDB](##Data-Augmentation-on-IMDB-dataset) and [SNLI](##Data-Augmentation-on-SNLI-dataset) datasets, to reproduce such a data-augmentation task, in order to improve our model, and decrease its variance when used in production with unseen data.\n",
    "2. We'll also try to [reproduce the results of the paper](##Reproducing-the-results), using similar models, to show how such a technique can be of key interest while working on a text-classification task.\n",
    "We'll use the data of the study, both IMDB and Stanford NLI, publicly available [here](https://github.com/acmi-lab/counterfactually-augmented-data).\n",
    "\n",
    "Additionally, for an overview of Kili, visit the [website](https://kili-technology.com), you can also check out the Kili [documentation](https://cloud.kili-technology.com/docs), or some other recipes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data augmentation](https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/data_collection_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication\n",
    "import os\n",
    "\n",
    "# !pip install kili # uncomment if you don't have kili installed already\n",
    "from kili.authentication import KiliAuth\n",
    "from kili.playground import Playground\n",
    "\n",
    "email = os.getenv('KILI_USER_EMAIL')\n",
    "password = os.getenv('KILI_USER_PASSWORD')\n",
    "api_endpoint = os.getenv('KILI_API_ENDPOINT') \n",
    "# If you use Kili SaaS, use the url 'https://cloud.kili-technology.com/api/label/graphql'\n",
    "\n",
    "kauth = KiliAuth(email=email, password=password, api_endpoint=api_endpoint)\n",
    "playground = Playground(kauth)\n",
    "user_id = kauth.user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation on IMDB dataset\n",
    "\n",
    "The data consists in reviews of films, that are classified as positives or negatives. State-of-the-art models performance is often measured against this dataset, making it a reference. \n",
    "\n",
    "This is how our task would look like on Kili, into 2 different projects for each task, from Positive to Negative or Negative to Positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskname = \"NEW_REVIEW\"\n",
    "project_imdb_negative_to_positive = {\n",
    "'title': 'Counterfactual data-augmentation - Negative to Positive',\n",
    "'description': 'IMDB Sentiment Analysis',\n",
    "'instructions': 'https://docs.google.com/document/d/1zhNaQrncBKc3aPKcnNa_mNpXlria28Ij7bfgUvJbyfw/edit?usp=sharing',\n",
    "'input_type': 'TEXT',\n",
    "'json_interface':{\n",
    "    \"filetype\": \"TEXT\",\n",
    "    \"jobRendererWidth\": 0.5,\n",
    "    \"jobs\": {\n",
    "        taskname : {\n",
    "            \"mlTask\": \"TRANSCRIPTION\",\n",
    "            \"content\": {\n",
    "                \"input\": None\n",
    "            },\n",
    "            \"required\": 1,\n",
    "            \"isChild\": False,\n",
    "            \"instruction\": \"Write here the new review modified to be POSITIVE. Please refer to the instructions above before starting\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "}\n",
    "project_imdb_positive_to_negative = {\n",
    "'title': 'Counterfactual data-augmentation - Positive to Negative',\n",
    "'description': 'IMDB Sentiment Analysis',\n",
    "'instructions': 'https://docs.google.com/document/d/1zhNaQrncBKc3aPKcnNa_mNpXlria28Ij7bfgUvJbyfw/edit?usp=sharing',\n",
    "'input_type': 'TEXT',\n",
    "'json_interface':{\n",
    "    \"jobRendererWidth\": 0.5,\n",
    "    \"jobs\": {\n",
    "        taskname : {\n",
    "            \"mlTask\": \"TRANSCRIPTION\",\n",
    "            \"content\": {\n",
    "                \"input\": None\n",
    "            },\n",
    "            \"required\": 1,\n",
    "            \"isChild\": False,\n",
    "            \"instruction\": \"Write here the new review modified to be NEGATIVE. Please refer to the instructions above before starting\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for project_imdb in [project_imdb_positive_to_negative,project_imdb_negative_to_positive] :\n",
    "    project_imdb['id'] = playground.create_empty_project(user_id=user_id)['id']\n",
    "    playground.update_properties_in_project(project_id=project_imdb['id'],\n",
    "                                            title=project_imdb['title'],\n",
    "                                            instructions=project_imdb['instructions'],\n",
    "                                            description=project_imdb['description'],\n",
    "                                            input_type=project_imdb['input_type'],\n",
    "                                            json_interface=project_imdb['json_interface'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll just create some useful functions for an improved readability :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_assets(dataframe, intro, objective, instructions, truth_label, target_label) :\n",
    "    return((intro + dataframe[truth_label] + objective + dataframe[target_label] + instructions + dataframe['Text']).tolist())\n",
    "\n",
    "def create_json_responses(taskname,df,field=\"Text\") :\n",
    "    return( [{taskname: { \"text\": df[field].iloc[k] }\n",
    "          } for k in range(df.shape[0]) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data into Kili"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "datasets = ['dev','train','test']\n",
    "\n",
    "for dataset in datasets :\n",
    "    url = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/sentiment/combined/paired/{dataset}_paired.tsv'\n",
    "    df = pd.read_csv(url, error_bad_lines=False, sep='\\t')\n",
    "    df = df[df.index%2 == 0] # keep only the original reviews as assets\n",
    "    \n",
    "    \n",
    "    for review_type,project_imdb in zip(['Positive','Negative'],[project_imdb_positive_to_negative,project_imdb_negative_to_positive]) :\n",
    "        dataframe = df[df['Sentiment']==review_type]\n",
    "        reviews_to_import = dataframe['Text'].tolist()\n",
    "        external_id_array = ('IMDB ' + review_type +' review ' + dataset + dataframe['batch_id'].astype('str')).tolist()\n",
    "    \n",
    "        playground.append_many_to_dataset(\n",
    "            project_id=project_imdb['id'],\n",
    "            content_array=reviews_to_import,\n",
    "            external_id_array=external_id_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the labels into Kili \n",
    "We will fill-in with the results of the study, as if they were predictions. In a real annotation project, we could fill in with the sentences as well so the labeler just has to write the changes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'results-arxiv:1909.12434'\n",
    "\n",
    "for dataset in datasets :\n",
    "    url = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/sentiment/combined/paired/{dataset}_paired.tsv'\n",
    "    df = pd.read_csv(url, error_bad_lines=False, sep='\\t')\n",
    "    df = df[df.index%2 == 1] # keep only the modified reviews as predictions\n",
    "    \n",
    "    for review_type,project_imdb in zip(['Positive','Negative'],[project_imdb_positive_to_negative,project_imdb_negative_to_positive]) :\n",
    "        dataframe = df[df['Sentiment']!=review_type]\n",
    "\n",
    "        external_id_array = ('IMDB ' + review_type +' review ' + dataset + dataframe['batch_id'].astype('str')).tolist()\n",
    "        json_response_array = create_json_responses(taskname,dataframe)\n",
    "    \n",
    "        playground.create_predictions(project_id=project_imdb['id'],\n",
    "            external_id_array=external_id_array,\n",
    "            model_name_array=[model_name]*len(external_id_array),\n",
    "            json_response_array=json_response_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how our interface looks in the end, allowing to quickly perform the task at hand\n",
    "\n",
    "![IMDB](./img/imdb_review.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation on SNLI dataset\n",
    "\n",
    "The data consists in a 3-class dataset, where, provided with two phrases, a premise and an hypothesis, the machine-learning task is to find the correct relation between those two sentences, that can be either entailment, contradiction or neutral.\n",
    "\n",
    "Here is an example of a premise, and three sentences that could be the hypothesis for the three categories :\n",
    "![examples](https://licor.me/post/img/robust-nlu/SNLI_annotation.png)\n",
    "\n",
    "This is how our task would look like on Kili, this time keeping it as a single project. To do so, we strongly remind the instructions at each labeler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskname = \"SENTENCE_MODIFIED\"\n",
    "project_snli={\n",
    "'title': 'Counterfactual data-augmentation NLI',\n",
    "'description': 'Stanford Natural language Inference',\n",
    "'instructions': '',\n",
    "'input_type': 'TEXT',\n",
    "'json_interface':{\n",
    "    \"jobRendererWidth\": 0.5,\n",
    "    \"jobs\": {\n",
    "        taskname: {\n",
    "            \"mlTask\": \"TRANSCRIPTION\",\n",
    "            \"content\": {\n",
    "                \"input\": None\n",
    "            },\n",
    "            \"required\": 1,\n",
    "            \"isChild\": False,\n",
    "            \"instruction\": \"Write here the modified sentence. Please refer to the instructions above before starting\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_snli['id'] = playground.create_empty_project(user_id=user_id)['id']\n",
    "print('Created project ' + project_snli[\"id\"])\n",
    "playground.update_properties_in_project(project_id=project_snli['id'],\n",
    "                                        title=project_snli['title'],\n",
    "                                        instructions=project_snli['instructions'],\n",
    "                                        description=project_snli['description'],\n",
    "                                        input_type=project_snli['input_type'],\n",
    "                                        json_interface=project_snli['json_interface'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll factorize our code a little, to merge datasets and differentiate properly all the cases of sentences : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(dataset, sentence_modified) :\n",
    "    url_original = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/NLI/original/{dataset}.tsv'\n",
    "    url_revised = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/NLI/revised_{sentence_modified}/{dataset}.tsv'\n",
    "    df_original = pd.read_csv(url_original, error_bad_lines=False, sep='\\t')\n",
    "    df_original = df_original[df_original.duplicated(keep='first')== False]\n",
    "    df_original['id'] = df_original.index.astype(str)\n",
    "    \n",
    "    df_revised = pd.read_csv(url_revised, error_bad_lines=False, sep='\\t')\n",
    "    axis_merge = 'sentence2' if sentence_modified=='premise' else 'sentence1'\n",
    "    # keep only one label per set of sentences\n",
    "    df_revised = df_revised[df_revised[[axis_merge,'gold_label']].duplicated(keep='first')== False]\n",
    "\n",
    "    df_merged = df_original.merge(df_revised, how='inner', left_on=axis_merge, right_on=axis_merge)\n",
    "    \n",
    "    if sentence_modified ==  'premise' :\n",
    "        df_merged['Text'] = df_merged['sentence1_x'] + '\\nSENTENCE 2 :\\n' + df_merged['sentence2']\n",
    "        instructions = \" relation, by making a small number of changes in the FIRST SENTENCE\\\n",
    "        such that the document remains coherent and the new label accurately describes the revised passage :\\n\\n\\n\\\n",
    "        SENTENCE 1 :\\n\"\n",
    "    else : \n",
    "        df_merged['Text'] = df_merged['sentence1'] + '\\nSENTENCE 2 :\\n' + df_merged['sentence2_x']\n",
    "        instructions = \" relation, by making a small number of changes in the SECOND SENTENCE\\\n",
    "        such that the document remains coherent and the new label accurately describes the revised passage :\\n\\n\\n\\\n",
    "        SENTENCE 1 : \\n\"\n",
    "    return(df_merged, instructions)\n",
    "\n",
    "def create_external_ids(dataset,dataframe, sentence_modified):\n",
    "    return(('NLI ' + dataset + ' ' + dataframe['gold_label_x'] + ' to ' + dataframe['gold_label_y'] + ' ' + sentence_modified + ' modified ' + dataframe['id']).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data into Kili\n",
    "We'll add before each set of sentences a small precision of the task for the labeler :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['dev','train','test']\n",
    "sentences_modified = ['premise', 'hypothesis']\n",
    "intro = \"Those two sentences' relation is classified as \"\n",
    "objective = \" to convert to a \"\n",
    "\n",
    "for dataset in datasets :\n",
    "    for sentence_modified in sentences_modified :\n",
    "        df,instructions = merge_datasets(dataset, sentence_modified)\n",
    "\n",
    "        sentences_to_import = create_assets(df, intro, objective, instructions, 'gold_label_x', 'gold_label_y')\n",
    "        external_id_array = create_external_ids(dataset, df, sentence_modified)\n",
    "    \n",
    "        playground.append_many_to_dataset(project_id=project_snli['id'],\n",
    "            content_array=sentences_to_import,\n",
    "            external_id_array=external_id_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the labels into Kili \n",
    "We will fill-in with the results of the study, as if they were predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'results-arxiv:1909.12434'\n",
    "\n",
    "for dataset in datasets :\n",
    "    for sentence_modified in sentences_modified :\n",
    "        axis_changed = 'sentence1_y' if sentence_modified=='premise' else 'sentence2_y'\n",
    "        df,instructions = merge_datasets(dataset, sentence_modified)\n",
    "\n",
    "        external_id_array = create_external_ids(dataset, df, sentence_modified)\n",
    "        json_response_array = create_json_responses(taskname,df,axis_changed) \n",
    "    \n",
    "        playground.create_predictions(project_id=project_snli['id'],\n",
    "            external_id_array=external_id_array,\n",
    "            model_name_array=[model_name]*len(external_id_array),\n",
    "            json_response_array=json_response_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLI](./img/snli_ex1.png)\n",
    "![NLI](./img/snli_ex2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing the results\n",
    "\n",
    "The study focus on comparing the performance of 5 models, when trained on the original dataset, or when trained on the new entire dataset.\n",
    "Those five models are SVM, Naïve Bayes, bi-LSTMs, ELMo-LSTM & BERT. \n",
    "### SVM & Naïve Bayes models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "maxlen=300\n",
    "datasets = ['train','dev', 'test']\n",
    "\n",
    "# Read train, validation and test data, and train TF-IDF matrix\n",
    "def prepare_dataset_imdb(which_data, for_sklearn=True) :\n",
    "    for dataset in datasets :\n",
    "        url = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/sentiment/combined/paired/{dataset}_paired.tsv'\n",
    "        df = pd.read_csv(url, error_bad_lines=False, sep='\\t')\n",
    "        if which_data == 'original' :\n",
    "            df = df[df.index%2 == 0] # keep only the original reviews\n",
    "        elif which_data == 'revised' :\n",
    "            df = df[df.index%2 == 1] # keep only the original reviews\n",
    "        if dataset == 'train':\n",
    "            y_train = df['Sentiment'].tolist()\n",
    "            if for_sklearn :\n",
    "                vec = TfidfVectorizer(decode_error='ignore', strip_accents='unicode', encoding='utf-8', min_df=10, max_df=1000)\n",
    "                X_train = vec.fit_transform(df['Text'])\n",
    "            else :\n",
    "                tokenizer = Tokenizer(num_words=20001,oov_token='UNK')\n",
    "                tokenizer.fit_on_texts(df['Text'].tolist())\n",
    "                print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "                sequences = tokenizer.texts_to_sequences(df['Text'].tolist())\n",
    "                X_train = pad_sequences(sequences, maxlen=maxlen)\n",
    "                y_train = np.array([int(y == 'Positive') for y in y_train]).reshape((-1,1))\n",
    "            print(\"Train matrix dimensionality: \", X_train.shape)\n",
    "        elif dataset == 'dev': \n",
    "            y_dev = df['Sentiment'].tolist()\n",
    "            if for_sklearn :\n",
    "                X_dev = vec.transform(df['Text'])\n",
    "            if not for_sklearn :\n",
    "                sequences = tokenizer.texts_to_sequences(df['Text'].tolist())\n",
    "                X_dev = pad_sequences(sequences, maxlen=maxlen)\n",
    "                y_dev = np.array([int(y == 'Positive') for y in y_dev]).reshape((-1,1))\n",
    "            print(\"Dev matrix dimensionality: \", X_dev.shape)\n",
    "        else :  \n",
    "            y_test = df['Sentiment'].tolist()\n",
    "            if for_sklearn :\n",
    "                X_test = vec.transform(df['Text'])\n",
    "            if not for_sklearn :\n",
    "                sequences = tokenizer.texts_to_sequences(df['Text'].tolist())\n",
    "                X_test = pad_sequences(sequences, maxlen=maxlen)\n",
    "                y_test = np.array([int(y == 'Positive') for y in y_test]).reshape((-1,1))\n",
    "            print(\"Test matrix dimensionality: \", X_test.shape)\n",
    "            return(X_train,y_train, X_dev,y_dev, X_test, y_test)\n",
    "\n",
    "\n",
    "# Use pipeline to search parameters for both TFIDF and SVM :\n",
    "#steps = [('TFIDF', StandardScaler()), ('SVM', SVC())]\n",
    "#from sklearn.pipeline import Pipeline\n",
    "#pipeline = Pipeline(steps) # define the pipeline object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy for SVC() : 0.9964850615114236\n",
      "Dev Accuracy for SVC() : 0.8775510204081632\n",
      "Test Accuracy for SVC() : 0.8442622950819673\n",
      "Train Accuracy for MultinomialNB() : 0.9086115992970123\n",
      "Dev Accuracy for MultinomialNB() : 0.8857142857142857\n",
      "Test Accuracy for MultinomialNB() : 0.8463114754098361\n"
     ]
    }
   ],
   "source": [
    "(X_train,y_train, X_dev,y_dev, X_test,y_test) = prepare_dataset_imdb('original')\n",
    "model_SVM = SVC()\n",
    "model_NaiveBayes = MultinomialNB()\n",
    "for model in (model_SVM,model_NaiveBayes) :\n",
    "    model.fit(X_train, y_train)\n",
    "    print(f'Train Accuracy for {model} : {accuracy_score(y_train,model.predict(X_train))}')\n",
    "    print(f'Dev Accuracy for {model} : {accuracy_score(y_dev,model.predict(X_dev))}')\n",
    "    print(f'Test Accuracy for {model} : {accuracy_score(y_test,model.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now run grid search to optimize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid_SVM = {\n",
    "    'kernel':('linear', 'rbf','poly'),\n",
    "    'C':[0.001,0.01,0.1,1,10,100,10e5],\n",
    "    'gamma':[0.1,0.01,0.01,0.001]}\n",
    "param_grid_NB  = parameters = {\n",
    "    'alpha': (10,1, 0.1, 0.01, 0.001, 0.0001, 0.00001)  \n",
    "    }\n",
    "for model,param_grid in zip((model_SVM,model_NaiveBayes),(param_grid_SVM,param_grid_NB)) :\n",
    "    grid = GridSearchCV(model, param_grid=param_grid, scoring='accuracy',cv=5, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "    #If there are multiple better models :\n",
    "    np.asarray(grid.cv_results_[0]['params'])[grid.cv_results_[0]['rank_test_score']==1]\n",
    "    print(\"score DEV = %3.2f\" %(grid.score(X_dev,y_dev)))\n",
    "    print(\"score TEST = %3.2f\" %(grid.score(X_test,y_test)))                  \n",
    "    print(grid.best_params_)    \n",
    "    print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from 'text-classification/models' import bi-LSTM\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, 'text-classification')\n",
    "import models.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19981 unique tokens.\n",
      "Train matrix dimensionality:  (1707, 300)\n",
      "Dev matrix dimensionality:  (245, 300)\n",
      "Test matrix dimensionality:  (488, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vocabulary_size = 20000\n",
    "maxlen = 300\n",
    "batch_size = 32\n",
    "lr = 1e-3\n",
    "\n",
    "(X_train_nn,y_train_nn, X_dev_nn,y_dev_nn, X_test_nn,y_test_nn) = prepare_dataset_imdb('original', for_sklearn=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Epoch 1/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 15.2492 - accuracy: 0.0000e+00"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    /usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:941 test_function  *\n        outputs = self.distribute_strategy.run(\n    <ipython-input-89-e0d6bd7b1bcc>:19 call  *\n        x = self.embedding(inputs)\n    /usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__  **\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/keras/layers/embeddings.py:181 call\n        dtype = K.dtype(inputs)\n    /usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:1268 dtype\n        return x.dtype.base_dtype.name\n\n    AttributeError: 'tuple' object has no attribute 'dtype'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-08cd0a37cd85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m model.fit(X_train_nn[0:10], y_train_nn[0:10],\n\u001b[0m\u001b[1;32m      7\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    860\u001b[0m           val_x, val_y, val_sample_weight = (\n\u001b[1;32m    861\u001b[0m               data_adapter.unpack_x_y_sample_weight(validation_data))\n\u001b[0;32m--> 862\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    863\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 step_num=step):\n\u001b[1;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 505\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    506\u001b[0m             *args, **kwds))\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: in user code:\n\n    /usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:941 test_function  *\n        outputs = self.distribute_strategy.run(\n    <ipython-input-89-e0d6bd7b1bcc>:19 call  *\n        x = self.embedding(inputs)\n    /usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__  **\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/keras/layers/embeddings.py:181 call\n        dtype = K.dtype(inputs)\n    /usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:1268 dtype\n        return x.dtype.base_dtype.name\n\n    AttributeError: 'tuple' object has no attribute 'dtype'\n"
     ]
    }
   ],
   "source": [
    "model = bi_LSTM()\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(X_train_nn[0:10], y_train_nn[0:10],\n",
    "          batch_size=batch_size,\n",
    "          epochs=20,\n",
    "          validation_data=[X_dev_nn, y_dev_nn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train...\n",
      "Train on 1707 samples, validate on 245 samples\n",
      "Epoch 1/20\n",
      "1707/1707 [==============================] - 11s 7ms/step - loss: 0.6932 - accuracy: 0.4927 - val_loss: 0.6931 - val_accuracy: 0.5020\n",
      "Epoch 2/20\n",
      "1707/1707 [==============================] - 10s 6ms/step - loss: 0.6932 - accuracy: 0.5026 - val_loss: 0.6930 - val_accuracy: 0.5143\n",
      "Epoch 3/20\n",
      "1707/1707 [==============================] - 11s 6ms/step - loss: 0.6713 - accuracy: 0.6766 - val_loss: 0.6643 - val_accuracy: 0.5959\n",
      "Epoch 4/20\n",
      "1707/1707 [==============================] - 11s 6ms/step - loss: 0.4763 - accuracy: 0.8055 - val_loss: 0.6893 - val_accuracy: 0.6082\n",
      "Epoch 5/20\n",
      "1707/1707 [==============================] - 11s 6ms/step - loss: 0.2923 - accuracy: 0.8729 - val_loss: 0.7719 - val_accuracy: 0.6571\n",
      "Epoch 6/20\n",
      "1707/1707 [==============================] - 11s 6ms/step - loss: 0.1713 - accuracy: 0.9432 - val_loss: 0.8751 - val_accuracy: 0.6776\n",
      "Epoch 7/20\n",
      "1707/1707 [==============================] - 11s 6ms/step - loss: 0.1219 - accuracy: 0.9572 - val_loss: 0.9711 - val_accuracy: 0.6939\n",
      "Epoch 8/20\n",
      "1707/1707 [==============================] - 10s 6ms/step - loss: 0.0879 - accuracy: 0.9660 - val_loss: 1.0783 - val_accuracy: 0.7102\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x143c5a880>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "bi_LSTM = Sequential()\n",
    "bi_LSTM.add(Embedding(20000, 50, input_length=300))\n",
    "#model.add(GlobalMaxPooling1D())\n",
    "bi_LSTM.add(Bidirectional(LSTM(50,recurrent_dropout=0.5,\n",
    "            recurrent_activation='tanh')))\n",
    "bi_LSTM.add(Dense(50, activation='relu'))\n",
    "bi_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = Adam(learning_rate=lr)\n",
    "bi_LSTM.compile(opt, 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "print('Train...')\n",
    "bi_LSTM.fit(X_train_nn, y_train_nn,\n",
    "          batch_size=32,callbacks=[callback],\n",
    "          epochs=20,\n",
    "          validation_data=[X_dev_nn, y_dev_nn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 300, 50)           1000000   \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 100)               40400     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,045,501\n",
      "Trainable params: 1,045,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
    "def ELMoEmbedding(input_text):\n",
    "    return elmo(tf.reshape(tf.cast(input_text, tf.string), [-1]), signature=\"default\", as_dict=True)[\"elmo\"]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(20000, 50, input_length=300))\n",
    "#model.add(GlobalMaxPooling1D())\n",
    "ELMo_LSTM.add(Bidirectional(LSTM(50,recurrent_dropout=0.5,\n",
    "            recurrent_activation='tanh')))\n",
    "ELMo_LSTM.add(Dense(50, activation='relu'))\n",
    "ELMo_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "opt = Adam(learning_rate=lr)\n",
    "ELMo_LSTM.compile(opt, 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "def build_model():\n",
    "    input_layer = Input(shape=(1,), dtype=\"string\", name=\"Input_layer\")\n",
    "    embedding_layer = Lambda(ELMoEmbedding, output_shape=(1024, ), name=\"Elmo_Embedding\")(input_layer)\n",
    "    BiLSTM = Bidirectional(layers.LSTM(1024, return_sequences= False, recurrent_dropout=0.2, dropout=0.2), name=\"BiLSTM\")(embedding_layer)\n",
    "    Dense_layer_1 = Dense(8336, activation='relu')(BiLSTM)\n",
    "    Dropout_layer_1 = Dropout(0.5)(Dense_layer_1)\n",
    "    Dense_layer_2 = Dense(4168, activation='relu')(Dropout_layer_1)\n",
    "    Dropout_layer_2 = Dropout(0.5)(Dense_layer_2)\n",
    "    output_layer = Dense(1, activation='sigmoid')(Dropout_layer_2)\n",
    "    model = Model(inputs=[input_layer], outputs=output_layer, name=\"BiLSTM with ELMo Embeddings\")\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "elmo_BiDirectional_model = build_model()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "print('Train...')\n",
    "ELMo_LSTM.fit(X_train_nn, y_train_nn,\n",
    "          batch_size=32,callbacks=[callback],\n",
    "          epochs=20,\n",
    "          validation_data=[X_dev_nn, y_dev_nn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class bi_LSTM(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, input_review_size=300, vocabulary_size=20000, n_word_embedding=50, n_hidden=50):\n",
    "        super(bi_LSTM, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            input_dim=vocabulary_size,\n",
    "            output_dim=n_word_embedding,\n",
    "            input_length=input_review_size)\n",
    "        self.max_pooling = tf.keras.layers.GlobalMaxPooling1D()\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            n_hidden,\n",
    "            recurrent_dropout=0.5,\n",
    "            recurrent_activation=tf.nn.relu)\n",
    "        self.bidirectional_lstm = tf.keras.layers.Bidirectional(self.lstm)\n",
    "        self.dense = tf.keras.layers.Dense(50, activation=tf.nn.relu)\n",
    "        self.out = tf.keras.layers.Dense(1, activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        #x = self.max_pooling(x)\n",
    "        x = self.bidirectional_lstm(x)\n",
    "        x = self.dense(x)\n",
    "        out = self.out(x)\n",
    "        return(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial, we learned how Kili can be a great help in your data augmentation task, as it allows to set a simple and easy to use interface, with proper instructions for your task.\n",
    "\n",
    "For the study, the quality of the labeling was a key feature in this complicated task, what Kili allows very simply. To monitor the quality of the results, we could set-up a consensus on a part or all of the annotations, or even keep a part of the dataset as ground truth to measure the performance of every labeler.\n",
    "\n",
    "For an overview of Kili, visit [kili-technology.com](https://kili-technology.com). You can also check out [Kili documentation](https://cloud.kili-technology.com/docs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
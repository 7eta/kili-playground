{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kili Tutorial: Leverage Counterfactually augmented data\n",
    "\n",
    "This recipe is inspired by the paper *Learning the Difference that Makes a Difference with Counterfactually-Augmented Data*, that you can find here on [arXiv](https://arxiv.org/abs/1909.12434)\n",
    "\n",
    "In this study, the authors point out the dificulty for Machine Learning models to generalize the classification rules learned, because their decision rules, described as 'spurious patternes', often miss the key elements that affects most the class of a text. They thus decided to delete what can be considered as a confusion factor, by changing the label of an asset at the same time as changing the minimum amount of words so those **key-words** would be much easier for the model to spot.\n",
    "\n",
    "We'll see in this tutorial how to create a project in Kili to reproduce such a data-augmentation task, in order to improve our model, and decrease its variance when used in production with unseen data.\n",
    "We'll use the data of the study, both IMDB and NLI, publicly available [here]().\n",
    "\n",
    "Additionally, for an overview of Kili, visit the [website](https://kili-technology.com), you can also check out the Kili [documentation](https://cloud.kili-technology.com/docs), or some other recipes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data augmentation](https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/data_collection_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authentication\n",
    "import os\n",
    "\n",
    "# !pip install kili # uncomment if you don't have kili installed already\n",
    "from kili.authentication import KiliAuth\n",
    "from kili.playground import Playground\n",
    "\n",
    "email = os.getenv('KILI_USER_EMAIL')\n",
    "password = os.getenv('KILI_USER_PASSWORD')\n",
    "api_endpoint = os.getenv('KILI_API_ENDPOINT') \n",
    "# If you use Kili SaaS, use the url 'https://cloud.kili-technology.com/api/label/graphql'\n",
    "\n",
    "kauth = KiliAuth(email=email, password=password, api_endpoint=api_endpoint)\n",
    "playground = Playground(kauth)\n",
    "user_id = kauth.user_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation on IMDB dataset :\n",
    "\n",
    "The data consists in reviews of films, that are classified as positives or negatives. State-of-the-art models performance is often measured against this dataset, making it a reference. \n",
    "\n",
    "This is how our task would look like on Kili :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskname = \"NEW_REVIEW\"\n",
    "project_imdb = {\n",
    "'title': 'Counterfactual data-augmentation IMDB',\n",
    "'description': 'IMDB Sentiment Analysis',\n",
    "'instructions': 'https://docs.google.com/document/d/1zhNaQrncBKc3aPKcnNa_mNpXlria28Ij7bfgUvJbyfw/edit?usp=sharing',\n",
    "'input_type': 'TEXT',\n",
    "'json_interface':{\n",
    "    \"filetype\": \"TEXT\",\n",
    "    \"jobRendererWidth\": 0.5,\n",
    "    \"jobs\": {\n",
    "        taskname : {\n",
    "            \"mlTask\": \"TRANSCRIPTION\",\n",
    "            \"content\": {\n",
    "                \"input\": None\n",
    "            },\n",
    "            \"required\": 1,\n",
    "            \"isChild\": False,\n",
    "            \"instruction\": \"Write here the review modified. Please refer to the instructions above before starting\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created project ckagwt8nf3ahw0972qi71fgye\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'ckagwt8nf3ahw0972qi71fgye'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_imdb['id'] = playground.create_empty_project(user_id=user_id)['id']\n",
    "print('Created project ' + project_imdb[\"id\"])\n",
    "playground.update_properties_in_project(project_id=project_imdb['id'],\n",
    "                                        title=project_imdb['title'],\n",
    "                                        instructions=project_imdb['instructions'],\n",
    "                                        description=project_imdb['description'],\n",
    "                                        input_type=project_imdb['input_type'],\n",
    "                                        json_interface=project_imdb['json_interface'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll just create some useful functions for an improved readability :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_assets(dataframe, intro, objective, instructions, truth_label, target_label) :\n",
    "    return((intro + dataframe[truth_label] + objective + dataframe[target_label] + instructions + dataframe['Text']).tolist())\n",
    "\n",
    "def create_json_responses(taskname,dataframe,field=\"Text\") :\n",
    "    return( [{taskname: { \"text\": dataframe[field].iloc[k] }\n",
    "          } for k in range(dataframe.shape[0]) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data into Kili\n",
    "We'll add before each review a small precision of the task for the labeler :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "datasets = ['dev', 'train','test']\n",
    "invert_sentiment = {'Positive' :'Negative', 'Negative': 'Positive' }\n",
    "intro = \"This review is a \"\n",
    "objective = \" review, to convert to a \"\n",
    "instructions = \" review, by making a small number of changes such that the document \\\n",
    "remains coherent and the new label accurately describes the revised passage :\\n\\n \"\n",
    "\n",
    "for dataset in datasets :\n",
    "    url = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/sentiment/combined/paired/{dataset}_paired.tsv'\n",
    "    df = pd.read_csv(url, error_bad_lines=False, sep='\\t')\n",
    "    df = df[df.index%2 == 0] # keep only the original reviews as assets\n",
    "    df['Target'] = df['Sentiment'].map(invert_sentiment)\n",
    "\n",
    "    reviews_to_import = create_assets(df, intro, objective, instructions, 'Sentiment', 'Target')\n",
    "    external_id_array = ('IMDB review ' + dataset + df['batch_id'].astype('str')).tolist()\n",
    "    \n",
    "    playground.append_many_to_dataset(\n",
    "        project_id=project_imdb['id'],\n",
    "        content_array=reviews_to_import,\n",
    "        external_id_array=external_id_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the labels into Kili \n",
    "We will fill-in with the results of the study, as if they were predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'results-arxiv:1909.12434'\n",
    "\n",
    "for dataset in datasets :\n",
    "    url = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/sentiment/combined/paired/{dataset}_paired.tsv'\n",
    "    df = pd.read_csv(url, error_bad_lines=False, sep='\\t')\n",
    "    df = df[df.index%2 == 1]\n",
    "    \n",
    "    external_id_array = ('IMDB review ' + dataset + df['batch_id'].astype('str')).tolist()\n",
    "    json_response_array = create_json_responses(taskname,df)\n",
    "    \n",
    "    playground.create_predictions(project_id=project_imdb['id'],\n",
    "        external_id_array=external_id_array,\n",
    "        model_name_array=[model_name]*len(external_id_array),\n",
    "        json_response_array=json_response_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how our interface looks in the end, allowing to quickly perform the task at hand\n",
    "\n",
    "![IMDB](./img/imdb_review.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation on Natural Language Inference dataset :\n",
    "\n",
    "The data consists in a 3-class dataset, where, provided with two phrases, a premise and an hypothesis, the task is to find the correct relation between those two sentences, that can be either entailment, contradiction or neutral.\n",
    "\n",
    "This is how our task would look like on Kili :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "taskname = \"SENTENCE_MODIFIED\"\n",
    "project_nli={\n",
    "'title': 'Counterfactual data-augmentation NLI',\n",
    "'description': 'Natural language Inference',\n",
    "'instructions': '',\n",
    "'input_type': 'TEXT',\n",
    "'json_interface':{\n",
    "    \"filetype\": \"TEXT\",\n",
    "    \"jobRendererWidth\": 0.5,\n",
    "    \"jobs\": {\n",
    "        taskname: {\n",
    "            \"mlTask\": \"TRANSCRIPTION\",\n",
    "            \"content\": {\n",
    "                \"input\": None\n",
    "            },\n",
    "            \"required\": 1,\n",
    "            \"isChild\": False,\n",
    "            \"instruction\": \"Write here the review modified. Please refer to the instructions above before starting\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created project ckah72lon0d7n0890teuxtbeh\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'ckah72lon0d7n0890teuxtbeh'}"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_nli['id'] = playground.create_empty_project(user_id=user_id)['id']\n",
    "print('Created project ' + project_nli[\"id\"])\n",
    "playground.update_properties_in_project(project_id=project_nli['id'],\n",
    "                                        title=project_nli['title'],\n",
    "                                        instructions=project_nli['instructions'],\n",
    "                                        description=project_nli['description'],\n",
    "                                        input_type=project_nli['input_type'],\n",
    "                                        json_interface=project_nli['json_interface'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we'll factorize our code a little, to merge datasets and differentiate properly all the cases of sentences : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_datasets(dataset, sentence_modified) :\n",
    "    url_original = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/NLI/original/{dataset}.tsv'\n",
    "    url_revised = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/NLI/revised_{sentence_modified}/{dataset}.tsv'\n",
    "    df_original = pd.read_csv(url_original, error_bad_lines=False, sep='\\t')\n",
    "    df_original = df_original[df_original.duplicated(keep='first')== False]\n",
    "    df_original['id'] = df_original.index.astype(str)\n",
    "    \n",
    "    df_revised = pd.read_csv(url_revised, error_bad_lines=False, sep='\\t')\n",
    "    axis_merge = 'sentence2' if sentence_modified=='premise' else 'sentence1'\n",
    "    # keep only one label per set of sentences\n",
    "    df_revised = df_revised[df_revised[[axis_merge,'gold_label']].duplicated(keep='first')== False]\n",
    "\n",
    "    df_merged = df_original.merge(df_revised, how='inner', left_on=axis_merge, right_on=axis_merge)\n",
    "    \n",
    "    if sentence_modified ==  'premise' :\n",
    "        df_merged['Text'] = df_merged['sentence1_x'] + '\\n' + df_merged['sentence2']\n",
    "        instructions = \" relation, by making a small number of changes in the FIRST SENTENCE\\\n",
    "        such that the document remains coherent and the new label accurately describes the revised passage :\\n\\n \"\n",
    "    else : \n",
    "        df_merged['Text'] = df_merged['sentence1'] + '\\n' + df_merged['sentence2_x']\n",
    "        instructions = \" relation, by making a small number of changes in the SECOND SENTENCE\\\n",
    "        such that the document remains coherent and the new label accurately describes the revised passage :\\n\\n \"\n",
    "    return(df_merged, instructions)\n",
    "\n",
    "def create_external_ids(dataset,dataframe, sentence_modified):\n",
    "    return(('NLI ' + dataset + ' ' + dataframe['gold_label_x'] + ' to ' + dataframe['gold_label_y'] + ' ' + sentence_modified + ' modified ' + dataframe['id']).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data into Kili\n",
    "We'll add before each set of sentences a small precision of the task for the labeler :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['dev', 'train','test']\n",
    "sentences_modified = ['premise', 'hypothesis']\n",
    "intro = \"Those two sentences' relation is classified as \"\n",
    "objective = \" to convert to a \"\n",
    "\n",
    "for dataset in datasets :\n",
    "    for sentence_modified in sentences_modified :\n",
    "        df,instructions = merge_datasets(dataset, sentence_modified)\n",
    "\n",
    "        sentences_to_import = create_assets(df, intro, objective, instructions, 'gold_label_x', 'gold_label_y')\n",
    "        external_id_array = create_external_ids(dataset, df, sentence_modified)\n",
    "    \n",
    "        playground.append_many_to_dataset(project_id=project_nli['id'],\n",
    "            content_array=sentences_to_import,\n",
    "            external_id_array=external_id_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the labels into Kili \n",
    "We will fill-in with the results of the study, as if they were predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'results-arxiv:1909.12434'\n",
    "\n",
    "for dataset in datasets :\n",
    "    for sentence_modified in sentences_modified :\n",
    "        axis_changed = 'sentence1_y' if sentence_modified=='premise' else 'sentence2_y'\n",
    "        df,instructions = merge_datasets(dataset, sentence_modified)\n",
    "\n",
    "        external_id_array = create_external_ids(dataset, df, sentence_modified)\n",
    "        json_response_array = create_json_responses(taskname,df,axis_changed) \n",
    "    \n",
    "        playground.create_predictions(project_id=project_nli['id'],\n",
    "            external_id_array=external_id_array,\n",
    "            model_name_array=[model_name]*len(external_id_array),\n",
    "            json_response_array=json_response_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NLI](./img/nli_ex1.png)\n",
    "![NLI](./img/nli_ex2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "In this tutorial, we learned how Kili can be a great help in your data augmentation task, as it allows to set a simple and easy to use interface, with proper instructions for your task.\n",
    "\n",
    "For the study, the quality of the labeling was a key feature in this complicated task, what Kili allows very simply. To monitor the quality of the results, we could set-up a consensus on a part or all of the annotations, or even keep a part of the dataset as ground truth to measure the performance of every labeler.\n",
    "\n",
    "For an overview of Kili, visit [kili-technology.com](https://kili-technology.com). You can also check out [Kili documentation](https://cloud.kili-technology.com/docs)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

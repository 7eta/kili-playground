{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient data augmentation for production-ready NLP tasks [1/3]\n",
    "\n",
    "As algorithm grow more complex, they also grow hungry for more data, to be able to learn precisely the meaning of the sentence. In Text Classification task for example, the variety of words encountered allows for a much more resilient algorithm, especially when in production with a taste of real word data. \n",
    "\n",
    "Just as in Computer Vision where biases such as detecting a seagull is more correlated with the presence of the beach rather than the bird itself, words in NLU can be baddly associated with an incorrect class and can thus be a threat in the case of real world use, as examples get harder to discriminate. \n",
    "If automatic generation of new data can be of help in simply **increasing the number of training example**, how good is it performing against the use of **more training data from the same dataset**, and **are there way to efficiently generate more data** ?\n",
    "\n",
    "This article is inspired by the paper *Learning the Difference that Makes a Difference with Counterfactually-Augmented Data*, that can be found here on [arXiv](https://arxiv.org/abs/1909.12434)\n",
    "\n",
    "In this study, the authors point out the difficulty for Machine Learning models to generalize the classification rules learned, because their decision rules, described as 'spurious patternes', often miss the key elements that affects most the sentiment of a text. They thus decided to  confusion factor, by changing the label of an asset at the same time as changing the minimum amount of words so those **key-words** would be much easier for the model to spot.\n",
    "\n",
    "We'll go through details of the paper for a text-classification task, and study \n",
    "1. The impact of counterfactually-augmented data \n",
    "2. Compare the efficiency and cost of such data generation technique\n",
    "\n",
    "We'll use the data of the study, the IMDB sentiment analysis dataset, publicly available [here](https://github.com/acmi-lab/counterfactually-augmented-data). \n",
    "The dataset consists in 50k reviews of movies, and the task is to classify those reviews as positive or negative opinions about a movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the article *Learning the Difference that Makes a Difference with Counterfactually-Augmented Data*, the authors created a new labelling task based on the IMDB dataset. On a subset of reviews, it was asked the annotators to change the class of the review by changing the minimum of words in the review. For example, the sentence : \n",
    "*Long, boring, blasphemous. Never have I been so glad to see ending credits roll.*\n",
    "became : *Long, fascinating, soulful. Never have I been so sad to see ending credits roll.* \n",
    "In the following, we will use those 4 datasets :\n",
    "- the *original* dataset, a subset from the imdb dataset\n",
    "- the *revised* dataset created in the study from variations of the original dataset\n",
    "- the *combined* dataset, combining the two previous datasets\n",
    "- the *originalDouble* dataset, enlarging the original dataset with more reviews from the imdb dataset.\n",
    "\n",
    "We will use in this study different machine learning models, and analyze their performance on the original data to understand how well counterfactual data can improve performance on this text classification task.\n",
    "\n",
    "### Experimental setup\n",
    "1. Pipeline\n",
    "\n",
    " - Each dataset, including the original IMDB dataset is split in a balanced way between a training, validation & test dataset.\n",
    " - For the text processing, no pre-processing is applied to the original reviews. \n",
    " - The reviews are encoded, for the machine learning models, with a TF-IDF bag-of-words approach. We first compute the term frequency :\n",
    "$$ \\mathrm {tf} (t,d)=0.5+0.5\\cdot {\\frac {f_{t,d}}{\\max\\{f_{t',d}:t'\\in d\\}}}$$\n",
    " And then the inverse document frequency : \n",
    "  $$\\mathrm{idf}(t, D) =  \\log \\frac{N}{|\\{d \\in D: t \\in d\\}|}$$\n",
    "\n",
    " \n",
    " - We then conduct a grid search on a dedicated validation dataset, specific to each model.\n",
    " - Final results are computed for all datasets, on the test set.\n",
    "\n",
    "2. Models\n",
    "\n",
    "We chose different enough models, to grab a better sense of the type of circonstances in which counterfactual data can be of help. The models tested are the sklearn implementation of : \n",
    " - SVM : A Support Vector Machine classifier.\n",
    " - RF : A Random Forest classifier\n",
    " - \n",
    "\n",
    "3. Metric\n",
    "\n",
    "The metric used is the **accuracy**, as the dataset is balanced it is not biased for this task.\n",
    "\n",
    "$$\\texttt{accuracy}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} 1_{(\\hat{y}_i = y_i)}$$\n",
    "\n",
    "\n",
    "### Building a more resilient dataset\n",
    "\n",
    "The interest of such a technic is primarily to help the model grab a better sense of the useful words that really encapture the meaning of the sentence. As a matter of fact, a model trained on the original data fails to score better than random guess on counterfactual out-of-sample data.\n",
    "The opposite is also true, a model trained on counterfactual data scores very poorly on original data, not better than random guess on out-of-sample data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset](./img/counterfactual_original_only.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This effect of resilience is well illustrated for the SVM or LogisticRegression classifiers, as it is possible to compute the feature importance of each word for the task. The comparison between the **key words** for the model trained on the combined dataset and for the original dataset is striking.\n",
    "\n",
    "The words that most contribute to the classification of the review are often not those that are most useful (words like `classic`, `one of`, `romantic`, or `something` for example)\n",
    "\n",
    "On the contrary, a model trained on the *combined* dataset (with **both the original and the revised dataset**), has much more coherent words as important features, increasing the ability of the model to classify correctly out-of-sample examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![features](./img/features_original.png)\n",
    "![features_comb](./img/features_comb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the **performance is much better for this last model**, on both the *revised* and the *original* dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual augmentation \n",
    "\n",
    "For a more accurate comparison, we choose to compare the performance of models with the same number of data. Below are gathered the results accross multiple models on the original test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3](./img/3,4k_reviews.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to differentiate 3 different situations in the results above : \n",
    "- For most models, the increase in data points increases performance. Those models are for example SVM, RF, DecisionTree, LogisticRegression, for which both models trained with more data score better on the original test set. \n",
    "- For some models, like Naive Bayes and K-Nearest Neighbors, which are simple models, performance is better for the model with counterfactually generated data.\n",
    "\n",
    "more detailled analysis per model :\n",
    "improves for KNN & NB (draw a conclusion)\n",
    "better than original 1,7 :\n",
    "CL :\n",
    "for simple models, allow to create sOTA\n",
    "not so obvious for hard models\n",
    "an hypothesis : models do not converges welll (see LSTM)\n",
    "\n",
    "It is not surprising to notice that **the model trained with more data perform generally better on the test data, accross every models**. Even so, the model trained on a larger original dataset does not always scores better, for example with Naive Bayes classifier or K-Nearest Neighbors, that perform better with the combined training dataset that gathers counterfactual and original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustify a model with counterfactual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another great interest for counterfactual data is that, when used with a fully parameterized model with a large training set, the performance increase is steady around 1% for almost all models tested here. \n",
    "\n",
    "Furthermore, as previously shown, the model obtained is now resilient towards counterfactual data, even with just the use of a small subset of counterfactual data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3](./img/20,7k_reviews.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost / efficiency analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate further the interest of the counterfactual annotation process, we decided to evaluate the difference in costs between collecting more data and creating counterfactual data. \n",
    "We thus reproduced the task of writing movie reviews, and present below the results of such a labelling task, for 100 words of reviews produced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing movie reviews\n",
    "The first project reproduced is the original IMDB task, onto the [Kili](https://kili-technology.com) plateform.\n",
    "\n",
    "Labelers are asked presented with a movie poster, and are asked to write a simple review positive or negative about the movie. Movie can be skipped if it is unknown. The dataset used came from two public IMDB best movies lists, and is interesting to not that the proportion of skipped movies was very high (Between one out of 8 movies and 1/3 of movies were known, even on a list of famous movies.)\n",
    "Below is a screenshot of Kili's overview interface in a project.\n",
    "\n",
    "![comparison](./img/efficient_data_augmentation/movie_reviews.png)\n",
    "\n",
    "Along the task, labelers can also quickly access extra synopsys information on the interface, to accelerate the task.\n",
    "\n",
    "Thus, we leveraged the strength of the annotation plateform to make the task as fast as possible, and we record for each review the **length of the review** produced, and the **time to produce the review**. \n",
    "\n",
    "![comparison](./img/efficient_data_augmentation/movie_review_example.png)\n",
    "\n",
    "\n",
    "### Producing counterfactual data\n",
    "\n",
    "In a second project, the same labelers are provided with original IMDB reviews, and are asked to produce counterfactual variation, by changing the smallest number of words possible. \n",
    "Again, we compute the time needed to produce the review, and the length of the review produced.\n",
    "\n",
    "![comparison](./img/efficient_data_augmentation/counterfactual_example.png)\n",
    "\n",
    "\n",
    "### Analyzing the results\n",
    "\n",
    "The figures are then exported through [Kili Playground](https://github.com/kili-technology/kili-playground/), and the distributions are represented below : in blue for counterfactual project, and in orange for original project.\n",
    "\n",
    "The figures are \n",
    "- for counterfactual data generation, the productivity is 11.9 ± 5.2 words/min\n",
    "- for original data generation, the productivity is 48.2 ± 30.6 words/min\n",
    "\n",
    "We apply a Z test to those two figures to compare the distribution, and find a \n",
    "\n",
    "![comparison](./img/efficient_data_augmentation/compare_distribs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are two mains interest to use counterfactual data augmentation. \n",
    "\n",
    "1. Its impact on the performance for text classification can be as good, and even better to using more original data. Furthermore, counterfactual data provides a resilience to the model towards data that can be considered as adversarial. \n",
    "\n",
    "2. Yet, the model of generation being so precise and systematic, it is **far easier to generate than collecting original data**. The original data gathering can as a matter of fact be really tedious and suffers from exhaustion, because it often relies on synthetic data, for examples in the chatbot use cases. In the beginning especially, we generate data syntetically to onboard the first users, but imagination has its limits. The counterfactual alternative guarantees, for real-world use case, a gain in factor 2 to 3 in terms of cost for the same final performance, and in quality of data as it does not suffer from the exhaustion issues.\n",
    "\n",
    "\n",
    "We studied here the case of text classification, but this could be done for many others fields of NLU, for examples Named entities recognition or intention classification. One of the primary use of this technic could thus be the development of chatbots, in which all those tasks are of primary concern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliography\n",
    "- TF-IDF : Luhn, Hans Peter (1957). \"A Statistical Approach to Mechanized Encoding and Searching of Literary Information\" & Spärck Jones, K. (1972). \"A Statistical Interpretation of Term Specificity and Its Application in Retrieval\"\n",
    "- Sklearn : Fabian Pedregosa, Gael Varoquaux et al, \"Scikit-learn: Machine Learning in Python\"(http://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro on text classif \n",
    "\n",
    "1. Counterfactual and results\n",
    "- Plus de données c'est mieux\n",
    "- le contrefactuel c'est mieux\n",
    "- le contrefactuel c'est débiaisé \n",
    "- Même avec beaucoup de données, c'est plus robuste vis à vis de données de production\n",
    "On fait tout cela sur l'analyse de sentiments mais étendable à d'autre tâches\n",
    "2. Cost/efficiency report counterfactual versus more data \n",
    "\n",
    "Graphe sur 100 phrases annotées counterfactual versus normalement\n",
    "- effet d'épuisement à montrer\n",
    "- idée de facteur 2 à 3 idéal\n",
    "\n",
    "Conclusions : \n",
    "- Donnée générée à la main est de moins bonne qualité (effet d'épuisement)\n",
    "- Works on other domains : for example Entités nommées classification d'intention - chatbot example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

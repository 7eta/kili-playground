{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient data augmentation for production-ready NLP tasks [1/3]\n",
    "\n",
    "As algorithm grow more complex, they also grow hungry for more data, to be able to learn precisely the meaning of the sentence. In Text Classification task for example, the variety of words encountered allows for a much more resilient algorithm, especially when in production with a taste of real word data. \n",
    "\n",
    "Just as in Computer Vision where biases such as detecting a seagull is more correlated with the presence of the beach rather than the bird itself, words in NLU can be baddly associated with an incorrect class and can thus be a threat in the case of real world use, as examples get harder to discriminate. \n",
    "If automatic generation of new data can be of help in simply **increasing the number of training example**, how good is it performing against the use of **more training data from the same dataset**, and **are there way to efficiently generate more data** ?\n",
    "\n",
    "This article is inspired by the paper *Learning the Difference that Makes a Difference with Counterfactually-Augmented Data*, that can be found here on [arXiv](https://arxiv.org/abs/1909.12434)\n",
    "\n",
    "In this study, the authors point out the difficulty for Machine Learning models to generalize the classification rules learned, because their decision rules, described as 'spurious patternes', often miss the key elements that affects most the sentiment of a text. They thus decided to  confusion factor, by changing the label of an asset at the same time as changing the minimum amount of words so those **key-words** would be much easier for the model to spot.\n",
    "\n",
    "We'll go through details of the paper for a text-classification task, and study \n",
    "1. The impact of counterfactually-augmented data \n",
    "2. Compare the efficiency and cost of such data generation technique\n",
    "\n",
    "We'll use the data of the study, the IMDB sentiment analysis dataset, publicly available [here](https://github.com/acmi-lab/counterfactually-augmented-data). \n",
    "The dataset consists in 50k reviews of movies, and the task is to classify those reviews as positive or negative opinions about a movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counterfactual data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the article *Learning the Difference that Makes a Difference with Counterfactually-Augmented Data*, the authors created a new labelling task based on the IMDB dataset. On a subset of reviews, it was asked the annotators to change the class of the review by changing the minimum of words in the review. For example, the sentence : \n",
    "*Long, boring, blasphemous. Never have I been so glad to see ending credits roll.*\n",
    "became : *Long, fascinating, soulful. Never have I been so sad to see ending credits roll.* \n",
    "In the following, we will use those 4 datasets :\n",
    "- the *original* dataset, a subset from the imdb dataset\n",
    "- the *revised* dataset created in the study from variations of the original dataset\n",
    "- the *combined* dataset, combining the two previous datasets\n",
    "- the *originalDouble* dataset, enlarging the original dataset with more reviews from the imdb dataset.\n",
    "\n",
    "We will use in this study different machine learning models, and analyse their performance on the original data to understand how well counterfactual data can improve performance on this text classification task.\n",
    "\n",
    "No pre-processing is applied to the text data, just a TF-IDF bag of words and a classifier, with a grid search on a dedicated validation dataset. All results are compared for an out-of-sample test set, always on balanced dataset. The metric used can thus be the accuracy.\n",
    "\n",
    "### Building a more resilient dataset\n",
    "\n",
    "The interest of such a technic is primarily to help the model grab a better sense of the useful words that really encapture the meaning of the sentence. As a matter of fact, a model trained on the original data fails to score better than random guess on counterfactual out-of-sample data.\n",
    "The opposite is also true, a model trained on counterfactual data scores very poorly on original data, not better than random guess on out-of-sample data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset](./img/counterfactual_original_only.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This effect of resilience is well illustrated for the SVM or LogisticRegression classifiers, as it is possible to compute the feature importance of each word for the task. The comparison between the **key words** for the model trained on the combined dataset and for the original dataset is striking.\n",
    "\n",
    "The words that most contribute to the classification of the review are often not those that are most useful (words like `classic`, `one of`, `romantic`, or `something` for example)\n",
    "\n",
    "On the contrary, a model trained on the *combined* dataset (with **both the original and the revised dataset**), has much more coherent words as important features, increasing the ability of the model to classify correctly out-of-sample examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![features](./img/features_original.png)\n",
    "![features_comb](./img/features_comb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the **performance is much better for this last model**, on both the *revised* and the *original* dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counterfactual augmentation \n",
    "\n",
    "For a more accurate comparison, we choose to compare the performance of models with the same number of data. Below are gathered the results accross multiple models on the original test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3](./img/3,4k_reviews.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not surprising to notice that **the model trained with more data perform generally better on the test data, accross every models**. Even so, the model trained on a larger original dataset does not always scores better, for example with Naive Bayes classifier or K-Nearest Neighbors, that perform better with the combined training dataset that gathers counterfactual and original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robustify a model with counterfactual data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another great interest for counterfactual data is that, when used with a fully parameterized model with a large training set, the performance increase is steady around 1% for almost all models tested here. \n",
    "\n",
    "Furthermore, as previously shown, the model obtained is now resilient towards counterfactual data, even with just the use of a small subset of counterfactual data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![3](./img/20,7k_reviews.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost / efficiency analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate further the interest of the counterfactual annotation process, we decided to evaluate the difference in costs between collecting more data and creating counterfactual data. \n",
    "We thus reproduced the task of writing movie reviews, and present below the results of such a labelling task, for 100 words of reviews produced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**graph to complete**\n",
    "- exhaustion\n",
    "- to to 3 in factor (times/100 words for each task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that there are two mains interest to use counterfactual data augmentation. \n",
    "\n",
    "1. Its impact on the performance for text classification can be as good, and even better to using more original data. Furthermore, counterfactual data provides a resilience to the model towards data that can be considered as adversarial. \n",
    "\n",
    "2. Yet, the model of generation being so precise and systematic, it is **far easier to generate than collecting original data**. The original data gathering can as a matter of fact be really tedious and suffers from exhaustion, because it often relies on synthetic data, for examples in the chatbot use cases. In the beginning especially, we generate data syntetically to onboard the first users, but imagination has its limits. The counterfactual alternative guarantees, for real-world use case, a gain in factor 2 to 3 in terms of cost for the same final performance, and in quality of data as it does not suffer from the exhaustion issues.\n",
    "\n",
    "\n",
    "We studied here the case of text classification, but this could be done for many others fields of NLU, for examples Named entities recognition or intention classification. One of the primary use of this technic could thus be the development of chatbots, in which all those tasks are of primary concern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro on text classif \n",
    "\n",
    "1. Counterfactual and results\n",
    "- Plus de donn√©es c'est mieux\n",
    "- le contrefactuel c'est mieux\n",
    "- le contrefactuel c'est d√©biais√© \n",
    "- M√™me avec beaucoup de donn√©es, c'est plus robuste vis √† vis de donn√©es de production\n",
    "On fait tout cela sur l'analyse de sentiments mais √©tendable √† d'autre t√¢ches\n",
    "2. Cost/efficiency report counterfactual versus more data \n",
    "\n",
    "Graphe sur 100 phrases annot√©es counterfactual versus normalement\n",
    "- effet d'√©puisement √† montrer\n",
    "- id√©e de facteur 2 √† 3 id√©al\n",
    "\n",
    "Conclusions : \n",
    "- Donn√©e g√©n√©r√©e √† la main est de moins bonne qualit√© (effet d'√©puisement)\n",
    "- Works on other domains : for example Entit√©s nomm√©es classification d'intention - chatbot example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "from nlpaug.flow.sometimes import Sometimes\n",
    "import os\n",
    "path_models = os.getenv('MODELS_PATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_models='/Users/philippedesaintchamas/ML_models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "The quick brown fox jumps over the lazy dog .\n"
     ]
    }
   ],
   "source": [
    "text = 'The quick brown fox jumps over the lazy dog .'\n",
    "\n",
    "## Keyboard distance\n",
    "#aug = nac.KeyboardAug()\n",
    "#augmented_text = aug.augment(text)\n",
    "print(\"Original:\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERT Augmentator\n",
    "TOPK=20 #default=100\n",
    "ACT = 'insert' #\"substitute\"\n",
    "\n",
    "aug_bert = naw.ContextualWordEmbsAug(\n",
    "    model_path='distilbert-base-uncased', \n",
    "    device='cuda',\n",
    "    action=ACT, top_k=TOPK)\n",
    "print(\"Original:\")\n",
    "print(text)\n",
    "print(\"Augmented Text:\")\n",
    "for ii in range(5):\n",
    "    augmented_text = aug_bert.augment(text)\n",
    "    print(augmented_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "synonym_aug = naw.SynonymAug(aug_src='wordnet')\n",
    "glove_aug = naw.WordEmbsAug(\n",
    "    #model_type='word2vec', model_path=path_models + 'GoogleNews-vectors-negative300',\n",
    "    model_type='glove', model_path=path_models + 'glove.6B.300d.txt',\n",
    "    action=\"substitute\")\n",
    "w2v_aug = naw.WordEmbsAug(\n",
    "    model_type='word2vec', model_path=path_models + 'GoogleNews-vectors-negative300',\n",
    "    #model_type='glove', model_path=path_models + 'glove.6B.300d.txt',\n",
    "    action=\"substitute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines= [Sometimes([nac.KeyboardAug(),synonym_aug, glove_aug], pipeline_p=0.1, aug_p=0.1),\n",
    "            Sometimes([nac.KeyboardAug(),synonym_aug, w2v_aug], pipeline_p=0.1, aug_p=0.1),\n",
    "            Sometimes([nac.KeyboardAug(),synonym_aug, glove_aug], pipeline_p=0.3, aug_p=0.1),\n",
    "            Sometimes([nac.KeyboardAug(),synonym_aug, glove_aug], pipeline_p=0.1, aug_p=0.3),\n",
    "            Sometimes([nac.KeyboardAug(),synonym_aug, w2v_aug], pipeline_p=0.2, aug_p=0.2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional,GlobalMaxPooling1D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "maxlen=300\n",
    "\n",
    "# Read train, validation and test data, and train TF-IDF matrix\n",
    "def prepare_dataset_imdb(which_data, for_sklearn=True, augmenter=None, number_copy=1) :\n",
    "    for dataset in ['train','dev'] :\n",
    "        url = f'https://raw.githubusercontent.com/acmi-lab/counterfactually-augmented-data/master/sentiment/combined/paired/{dataset}_paired.tsv'\n",
    "        df = pd.read_csv(url, error_bad_lines=False, sep='\\t')\n",
    "        \n",
    "        # Split data on original/counterfactual\n",
    "        if which_data == 'counterfactual' :\n",
    "            _df = df[df.index%2 == 0] \n",
    "            df = df[df.index%2 == 1] # keep only the revised reviews\n",
    "        else :\n",
    "            df = df[df.index%2 == 0] # keep only the original reviews\n",
    "            _df = df.copy(deep=True)\n",
    "\n",
    "        # Data augmentation\n",
    "        if which_data =='augmented':\n",
    "            for text,label,_id in tqdm(zip(_df['Text'].tolist(),_df['Sentiment'].tolist(),_df['batch_id'].tolist())) :\n",
    "                augmented_texts = augmenter.augment(text,n=number_copy)\n",
    "                if type(augmented_texts) != list :\n",
    "                    augmented_texts = [augmented_texts]\n",
    "                for new_text in augmented_texts :\n",
    "                    if new_text != text :\n",
    "                        df = df.append({'Text' : new_text , 'Sentiment' : label, 'batch_id': _id} , ignore_index=True)\n",
    "        \n",
    "        if dataset == 'train':\n",
    "            y_train = df['Sentiment'].tolist()\n",
    "            if for_sklearn :\n",
    "                vec = TfidfVectorizer(decode_error='ignore', strip_accents='unicode', encoding='utf-8', min_df=10, max_df=500)\n",
    "                vec.fit(_df['Text'])\n",
    "                X_train = vec.transform(df['Text']) # fit and transform are made on different datasets\n",
    "            else :\n",
    "                tokenizer = Tokenizer(num_words=20001,oov_token='UNK')\n",
    "                tokenizer.fit_on_texts(_df['Text'].tolist())\n",
    "                print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "                sequences = tokenizer.texts_to_sequences(df['Text'].tolist())\n",
    "                X_train = pad_sequences(sequences, maxlen=maxlen)\n",
    "                y_train = np.array([int(y == 'Positive') for y in y_train]).reshape((-1,1))\n",
    "            print(\"Train matrix dimensionality: \", X_train.shape)\n",
    "        elif dataset == 'dev': \n",
    "            y_dev = df['Sentiment'].tolist()\n",
    "            if for_sklearn :\n",
    "                X_dev = vec.transform(df['Text'])\n",
    "            if not for_sklearn :\n",
    "                sequences = tokenizer.texts_to_sequences(df['Text'].tolist())\n",
    "                X_dev = pad_sequences(sequences, maxlen=maxlen)\n",
    "                y_dev = np.array([int(y == 'Positive') for y in y_dev]).reshape((-1))\n",
    "            print(\"Dev matrix dimensionality: \", X_dev.shape)\n",
    "\n",
    "    return({'X_train':X_train, 'y_train':y_train, 'X_dev':X_dev,'y_dev':y_dev})\n",
    "\n",
    "def print_results(original, augmented) :\n",
    "    print(f\"Train Accuracy augmented: {accuracy_score(augmented['y_train'], augmented['model'].predict(augmented['X_train']))}\")\n",
    "    print(f\"Train Accuracy original : {accuracy_score(original['y_train'], original['model'].predict(original['X_train']))}\")\n",
    "    print(f\"Train Accuracy augmented on original data : {accuracy_score(original['y_train'], augmented['model'].predict(original['X_train']))}\")\n",
    "    print(f\"Dev Accuracy augmented : {accuracy_score(augmented['y_dev'], augmented['model'].predict(augmented['X_dev']))}\")\n",
    "    print(f\"Dev Accuracy original: {accuracy_score(original['y_dev'], original['model'].predict(original['X_dev']))}\")\n",
    "    print(f\"Dev Accuracy augmented on original data: {accuracy_score(original['y_dev'], augmented['model'].predict(original['X_dev']))}\")\n",
    "    return True\n",
    "\n",
    "def print_results_nn(original, augmented) :\n",
    "    print(f\"Train Accuracy augmented: {accuracy_score(augmented['y_train'], np.round(augmented['model'].predict(augmented['X_train'])))}\")\n",
    "    print(f\"Train Accuracy original : {accuracy_score(original['y_train'], np.round(original['model'].predict(original['X_train'])))}\")\n",
    "    print(f\"Train Accuracy augmented on original data : {accuracy_score(original['y_train'], np.round(augmented['model'].predict(original['X_train'])))}\")\n",
    "    print(f\"Dev Accuracy augmented : {accuracy_score(augmented['y_dev'], np.round(augmented['model'].predict(augmented['X_dev'])))}\")\n",
    "    print(f\"Dev Accuracy original: {accuracy_score(original['y_dev'], np.round(original['model'].predict(original['X_dev'])))}\")\n",
    "    print(f\"Dev Accuracy augmented on original data: {accuracy_score(original['y_dev'], np.round(augmented['model'].predict(original['X_dev'])))}\")\n",
    "    return True\n",
    "\n",
    "def train_models(model,list_dict_data, percentage_train=1) :\n",
    "    \"\"\"\n",
    "    Train, and print learning curve if NN\n",
    "    \"\"\"\n",
    "    list_dict_data_models = []\n",
    "    for dataset in list_dict_data :\n",
    "        if model == 'biLSTM' :\n",
    "            bi_LSTM = Sequential()\n",
    "            bi_LSTM.add(Embedding(20000, 50, input_length=300))\n",
    "            #model.add(GlobalMaxPooling1D())\n",
    "            bi_LSTM.add(Bidirectional(LSTM(50,recurrent_dropout=0.5,recurrent_activation='tanh')))\n",
    "            bi_LSTM.add(Dense(50, activation='relu'))\n",
    "            bi_LSTM.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "            opt = Adam(learning_rate=1e-3)\n",
    "            bi_LSTM.compile(opt, 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "            print('Train...')\n",
    "            history = bi_LSTM.fit(dataset['X_train'], dataset['y_train'],batch_size=32, callbacks=[callback],\n",
    "          epochs=20,validation_data=[dataset['X_dev'], dataset['y_dev']])\n",
    "            dataset['model'] = bi_LSTM\n",
    "            dataset['history'] = history\n",
    "        else :\n",
    "            history = model.fit(dataset['X_train'], dataset['y_train'])\n",
    "            dataset['model'] = model\n",
    "            dataset['history'] = history\n",
    "        list_dict_data_models.append(dataset)\n",
    "    return list_dict_data_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix dimensionality:  (1707, 2173)\n",
      "Dev matrix dimensionality:  (245, 2173)\n",
      "Train matrix dimensionality:  (1707, 2173)\n",
      "Dev matrix dimensionality:  (245, 2173)\n",
      "Train Accuracy augmented: 0.9988283538371412\n",
      "Train Accuracy original : 0.2032806092560047\n",
      "Train Accuracy augmented on original data : 0.2032806092560047\n",
      "Dev Accuracy augmented : 0.889795918367347\n",
      "Dev Accuracy original: 0.5142857142857142\n",
      "Dev Accuracy augmented on original data: 0.5142857142857142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1707it [05:22,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix dimensionality:  (2966, 2173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [00:52,  4.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev matrix dimensionality:  (426, 2173)\n",
      "Train Accuracy augmented: 0.9996628455832771\n",
      "Train Accuracy original : 0.2032806092560047\n",
      "Train Accuracy augmented on original data : 0.9994141769185706\n",
      "Dev Accuracy augmented : 0.8661971830985915\n",
      "Dev Accuracy original: 0.5142857142857142\n",
      "Dev Accuracy augmented on original data: 0.8653061224489796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1707it [25:06,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix dimensionality:  (2930, 2173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [03:11,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev matrix dimensionality:  (419, 2173)\n",
      "Train Accuracy augmented: 0.9989761092150171\n",
      "Train Accuracy original : 0.2032806092560047\n",
      "Train Accuracy augmented on original data : 0.9982425307557118\n",
      "Dev Accuracy augmented : 0.8568019093078759\n",
      "Dev Accuracy original: 0.5142857142857142\n",
      "Dev Accuracy augmented on original data: 0.8612244897959184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1707it [09:23,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix dimensionality:  (3387, 2173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [01:16,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev matrix dimensionality:  (487, 2173)\n",
      "Train Accuracy augmented: 1.0\n",
      "Train Accuracy original : 0.2032806092560047\n",
      "Train Accuracy augmented on original data : 1.0\n",
      "Dev Accuracy augmented : 0.864476386036961\n",
      "Dev Accuracy original: 0.5142857142857142\n",
      "Dev Accuracy augmented on original data: 0.8612244897959184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1707it [05:07,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix dimensionality:  (2940, 2173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [00:37,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev matrix dimensionality:  (431, 2173)\n",
      "Train Accuracy augmented: 0.9996598639455783\n",
      "Train Accuracy original : 0.2032806092560047\n",
      "Train Accuracy augmented on original data : 0.9994141769185706\n",
      "Dev Accuracy augmented : 0.877030162412993\n",
      "Dev Accuracy original: 0.5142857142857142\n",
      "Dev Accuracy augmented on original data: 0.8653061224489796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1707it [1:42:36,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train matrix dimensionality:  (3303, 2173)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [08:32,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev matrix dimensionality:  (477, 2173)\n",
      "Train Accuracy augmented: 1.0\n",
      "Train Accuracy original : 0.2032806092560047\n",
      "Train Accuracy augmented on original data : 1.0\n",
      "Dev Accuracy augmented : 0.8553459119496856\n",
      "Dev Accuracy original: 0.5142857142857142\n",
      "Dev Accuracy augmented on original data: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "original = prepare_dataset_imdb('original', for_sklearn=True)\n",
    "counterfactual = prepare_dataset_imdb('counterfactual', for_sklearn=True)\n",
    "[original,counterfactual] = train_models(SVC(),[original,counterfactual], percentage_train=1)\n",
    "print_results(original, counterfactual)\n",
    "for pipeline in pipelines :\n",
    "    augmented = prepare_dataset_imdb('augmented', for_sklearn=True, augmenter= pipeline)\n",
    "    [augmented] = train_models(SVC(),[augmented], percentage_train=1)\n",
    "    print_results(original, augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19981 unique tokens.\n",
      "Train matrix dimensionality:  (1707, 300)\n",
      "Dev matrix dimensionality:  (245, 300)\n",
      "Found 19981 unique tokens.\n",
      "Train matrix dimensionality:  (1707, 300)\n",
      "Dev matrix dimensionality:  (245, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.7/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.7/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy augmented: 0.8705330990041008\n",
      "Train Accuracy original : 0.29349736379613356\n",
      "Train Accuracy augmented on original data : 0.29349736379613356\n",
      "Dev Accuracy augmented : 0.5183673469387755\n",
      "Dev Accuracy original: 0.4816326530612245\n",
      "Dev Accuracy augmented on original data: 0.4816326530612245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1707it [07:13,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19981 unique tokens.\n",
      "Train matrix dimensionality:  (2961, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [00:55,  4.45it/s]\n",
      "/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.7/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev matrix dimensionality:  (406, 300)\n",
      "Name:Sometimes_Pipeline, Action:sometimes, Method:flow\n",
      "Train Accuracy augmented: 0.8682877406281662\n",
      "Train Accuracy original : 0.29349736379613356\n",
      "Train Accuracy augmented on original data : 0.8711189220855302\n",
      "Dev Accuracy augmented : 0.5640394088669951\n",
      "Dev Accuracy original: 0.4816326530612245\n",
      "Dev Accuracy augmented on original data: 0.5469387755102041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1707it [38:09,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19981 unique tokens.\n",
      "Train matrix dimensionality:  (2936, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "245it [12:21,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev matrix dimensionality:  (417, 300)\n",
      "Name:Sometimes_Pipeline, Action:sometimes, Method:flow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Caskroom/miniconda/base/envs/kilienv/lib/python3.7/site-packages/sklearn/utils/validation.py:73: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy augmented: 0.8671662125340599\n",
      "Train Accuracy original : 0.29349736379613356\n",
      "Train Accuracy augmented on original data : 0.872290568248389\n",
      "Dev Accuracy augmented : 0.5539568345323741\n",
      "Dev Accuracy original: 0.4816326530612245\n",
      "Dev Accuracy augmented on original data: 0.5346938775510204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1587it [09:13,  2.33it/s]"
     ]
    }
   ],
   "source": [
    "original = prepare_dataset_imdb('original', for_sklearn=False)\n",
    "counterfactual = prepare_dataset_imdb('counterfactual', for_sklearn=False)\n",
    "[original,counterfactual] = train_models(SVC(),[original,counterfactual], percentage_train=1)\n",
    "print_results_nn(original, counterfactual)\n",
    "for pipeline in pipelines :\n",
    "    augmented = prepare_dataset_imdb('augmented', for_sklearn=False, augmenter= pipeline)\n",
    "    print(pipeline)\n",
    "    [augmented] = train_models(SVC(),[augmented], percentage_train=1)\n",
    "    print_results_nn(original, augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( new, open( \"save.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented = prepare_dataset_imdb('original', for_sklearn=False, create_variation=True, augmenter= pipeline)\n",
    "counterfactual = prepare_dataset_imdb('revised', for_sklearn=False, create_variation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = prepare_dataset_imdb('original', for_sklearn=False, create_variation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_models('biLSTM',original,augmented, percentage_train=1)\n",
    "train_models('biLSTM',original,augmented, percentage_train=1)\n",
    "print_results(original, augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
